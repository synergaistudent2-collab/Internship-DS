{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Fundamentals: Supervised vs. Unsupervised Learning\n",
    "\n",
    "**Objective:** Understand the difference between supervised and unsupervised learning, and learn how to evaluate models using core metrics.\n",
    "\n",
    "**Author:** ML Fundamentals Tutorial  \n",
    "**Level:** Intern/Beginner  \n",
    "**Date:** 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Setup & Imports](#setup)\n",
    "3. [Datasets](#datasets)\n",
    "4. [Supervised Learning Example](#supervised)\n",
    "5. [Unsupervised Learning Example](#unsupervised)\n",
    "6. [Model Evaluation Metrics](#metrics)\n",
    "7. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id='introduction'></a>\n",
    "\n",
    "### What is Supervised Learning?\n",
    "\n",
    "**Supervised learning** involves training a model on labeled data, where each input has a known output (target/label). The model learns to map inputs to outputs and can then predict labels for new, unseen data.\n",
    "\n",
    "**Examples:**\n",
    "- **Classification:** Predicting discrete categories (e.g., spam vs. not spam, disease diagnosis)\n",
    "- **Regression:** Predicting continuous values (e.g., house prices, temperature)\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "**Unsupervised learning** works with unlabeled data. The model tries to find patterns, structure, or groupings in the data without any predefined labels.\n",
    "\n",
    "**Examples:**\n",
    "- **Clustering:** Grouping similar data points (e.g., customer segmentation, image compression)\n",
    "- **Dimensionality Reduction:** Reducing feature space while preserving information (e.g., PCA, t-SNE)\n",
    "\n",
    "### Train/Validation/Test Split\n",
    "\n",
    "When building machine learning models, we split data into:\n",
    "- **Training set:** Used to train the model (typically 60-80%)\n",
    "- **Validation set:** Used to tune hyperparameters (optional, 10-20%)\n",
    "- **Test set:** Used to evaluate final model performance (10-20%)\n",
    "\n",
    "**Why?** This prevents **overfitting** (model memorizes training data) and ensures the model generalizes well to new data.\n",
    "\n",
    "### Why Model Evaluation Matters\n",
    "\n",
    "Evaluation metrics help us:\n",
    "1. Assess model performance objectively\n",
    "2. Compare different models\n",
    "3. Understand where the model succeeds or fails\n",
    "4. Make informed decisions about model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Imports <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    adjusted_rand_score\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Datasets <a id='datasets'></a>\n",
    "\n",
    "We'll use scikit-learn's built-in datasets:\n",
    "- **Breast Cancer Dataset:** For supervised classification (binary: malignant vs. benign)\n",
    "- **Iris Dataset:** For unsupervised clustering (3 flower species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset for supervised learning\n",
    "cancer_data = datasets.load_breast_cancer()\n",
    "X_cancer = cancer_data.data\n",
    "y_cancer = cancer_data.target\n",
    "\n",
    "print(\"Breast Cancer Dataset:\")\n",
    "print(f\"  Samples: {X_cancer.shape[0]}\")\n",
    "print(f\"  Features: {X_cancer.shape[1]}\")\n",
    "print(f\"  Classes: {cancer_data.target_names}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_cancer)}\\n\")\n",
    "\n",
    "# Load Iris dataset for unsupervised learning\n",
    "iris_data = datasets.load_iris()\n",
    "X_iris = iris_data.data\n",
    "y_iris_true = iris_data.target  # We'll use this only for evaluation\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"  Samples: {X_iris.shape[0]}\")\n",
    "print(f\"  Features: {X_iris.shape[1]}\")\n",
    "print(f\"  Classes: {iris_data.target_names}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_iris_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised Learning Example <a id='supervised'></a>\n",
    "\n",
    "### Classification Task: Breast Cancer Diagnosis\n",
    "\n",
    "We'll build a binary classifier to predict whether a tumor is **malignant (0)** or **benign (1)** based on cell measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer, y_cancer, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_cancer  # Maintain class distribution in splits\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=10000, random_state=RANDOM_STATE)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "print(\"✓ Model trained successfully!\")\n",
    "print(f\"First 10 predictions: {y_pred[:10]}\")\n",
    "print(f\"First 10 true labels: {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation Metrics <a id='metrics'></a>\n",
    "\n",
    "### Key Metrics Explained\n",
    "\n",
    "#### 1. Accuracy\n",
    "**Definition:** Proportion of correct predictions out of total predictions.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$\n",
    "\n",
    "**When to use:** Good for balanced datasets where all classes are equally important.\n",
    "\n",
    "**Limitation:** Misleading with imbalanced classes (e.g., 95% of data is class A, predicting all A gives 95% accuracy).\n",
    "\n",
    "#### 2. Precision\n",
    "**Definition:** Of all positive predictions, how many were actually correct?\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$\n",
    "\n",
    "**When to use:** When **false positives are costly** (e.g., spam detection—you don't want legitimate emails marked as spam).\n",
    "\n",
    "#### 3. Recall (Sensitivity)\n",
    "**Definition:** Of all actual positives, how many did we correctly identify?\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n",
    "\n",
    "**When to use:** When **false negatives are costly** (e.g., disease diagnosis—missing a sick patient is critical).\n",
    "\n",
    "#### 4. Confusion Matrix\n",
    "A table showing true vs. predicted labels:\n",
    "\n",
    "|                | Predicted Negative | Predicted Positive |\n",
    "|----------------|--------------------|--------------------||\n",
    "| **Actual Negative** | True Negative (TN) | False Positive (FP) |\n",
    "| **Actual Positive** | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "#### Precision-Recall Tradeoff\n",
    "- **High Precision, Low Recall:** Model is conservative (only predicts positive when very confident)\n",
    "- **Low Precision, High Recall:** Model is aggressive (predicts positive more liberally)\n",
    "- Balance depends on the problem domain and cost of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SUPERVISED LEARNING - MODEL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Display detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer_data.target_names,\n",
    "            yticklabels=cancer_data.target_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Breast Cancer Classification', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract confusion matrix values for interpretation\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (TN):  {tn}\")\n",
    "print(f\"  False Positives (FP): {fp}\")\n",
    "print(f\"  False Negatives (FN): {fn}\")\n",
    "print(f\"  True Positives (TP):  {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "**Model Performance Commentary:**\n",
    "\n",
    "1. **Accuracy:** Our model correctly classifies most samples. However, accuracy alone doesn't tell the full story, especially in medical diagnosis.\n",
    "\n",
    "2. **Precision:** High precision means when the model predicts \"benign,\" it's usually correct. Few false alarms.\n",
    "\n",
    "3. **Recall:** High recall means we catch most actual benign cases. In medical diagnosis, we especially care about not missing malignant cases (the negative class here).\n",
    "\n",
    "4. **Class Imbalance Considerations:**\n",
    "   - The Breast Cancer dataset is relatively balanced (~63% benign, ~37% malignant)\n",
    "   - In highly imbalanced scenarios (e.g., 99:1 ratio), accuracy becomes less meaningful\n",
    "   - We'd rely more on precision, recall, F1-score, or area under ROC curve\n",
    "   - Consider using class weights or resampling techniques for severe imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unsupervised Learning Example <a id='unsupervised'></a>\n",
    "\n",
    "### Clustering Task: Iris Species Grouping\n",
    "\n",
    "We'll use **K-Means clustering** to group iris flowers based on their measurements, **without using the species labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means clustering (k=3 since we know there are 3 species)\n",
    "# In real unsupervised scenarios, we'd use techniques like the elbow method to find k\n",
    "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_iris)\n",
    "\n",
    "print(\"✓ K-Means clustering completed!\")\n",
    "print(f\"Cluster centers shape: {kmeans.cluster_centers_.shape}\")\n",
    "print(f\"Cluster distribution: {np.bincount(cluster_labels)}\")\n",
    "print(f\"\\nFirst 10 cluster assignments: {cluster_labels[:10]}\")\n",
    "print(f\"First 10 true species labels: {y_iris_true[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Unsupervised Learning\n",
    "\n",
    "**Challenge:** In true unsupervised learning, we don't have labels, so traditional metrics (accuracy, precision, recall) don't apply directly.\n",
    "\n",
    "**Approaches:**\n",
    "1. **Internal metrics:** Silhouette score, inertia (sum of squared distances to cluster centers)\n",
    "2. **External metrics (when labels available for validation):** Adjusted Rand Index, Normalized Mutual Information\n",
    "3. **Visual inspection:** Plot clusters in 2D/3D space\n",
    "\n",
    "**Important Note:** We're using true labels here only for educational purposes to see how well clusters align with actual species. In production, you wouldn't have these labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clusters to true labels using Adjusted Rand Index\n",
    "# ARI measures similarity between two clusterings (ranges from -1 to 1, 1 = perfect match)\n",
    "ari_score = adjusted_rand_score(y_iris_true, cluster_labels)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"UNSUPERVISED LEARNING - CLUSTER EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Adjusted Rand Index: {ari_score:.4f}\")\n",
    "print(\"  (1.0 = perfect clustering, 0.0 = random clustering)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a cross-tabulation to see cluster-to-species mapping\n",
    "print(\"\\nCluster vs. True Species Cross-Tabulation:\")\n",
    "crosstab = pd.crosstab(\n",
    "    pd.Series(y_iris_true, name='True Species'),\n",
    "    pd.Series(cluster_labels, name='Cluster'),\n",
    "    margins=True\n",
    ")\n",
    "# Map numeric labels to species names\n",
    "crosstab.index = [iris_data.target_names[i] if isinstance(i, (int, np.integer)) else i \n",
    "                  for i in crosstab.index]\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 2D using PCA for visualization\n",
    "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_iris_2d = pca.fit_transform(X_iris)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Clusters from K-Means\n",
    "scatter1 = axes[0].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], \n",
    "                           c=cluster_labels, cmap='viridis', \n",
    "                           s=50, alpha=0.6, edgecolors='black')\n",
    "axes[0].scatter(pca.transform(kmeans.cluster_centers_)[:, 0], \n",
    "                pca.transform(kmeans.cluster_centers_)[:, 1],\n",
    "                c='red', marker='X', s=200, edgecolors='black', \n",
    "                label='Centroids', linewidths=2)\n",
    "axes[0].set_title('K-Means Clusters (Unsupervised)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2: True species labels (for comparison)\n",
    "scatter2 = axes[1].scatter(X_iris_2d[:, 0], X_iris_2d[:, 1], \n",
    "                           c=y_iris_true, cmap='viridis', \n",
    "                           s=50, alpha=0.6, edgecolors='black')\n",
    "axes[1].set_title('True Species Labels (Ground Truth)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter2, ax=axes[1], ticks=[0, 1, 2])\n",
    "cbar.set_label('Species')\n",
    "cbar.ax.set_yticklabels(iris_data.target_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Clustering Results\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "1. **Visual Comparison:** By comparing the two plots, we can see how well K-Means discovered the natural groupings in the data without any label information.\n",
    "\n",
    "2. **Adjusted Rand Index:** This score tells us the agreement between clusters and true species. A high score indicates K-Means found meaningful patterns that align with biological species.\n",
    "\n",
    "3. **Limitations of Applying Supervised Metrics:**\n",
    "   - **Cluster labels are arbitrary:** Cluster 0 might correspond to species 2, cluster 1 to species 0, etc.\n",
    "   - **No notion of \"correct\" in pure unsupervised learning:** We can't calculate accuracy/precision/recall without ground truth labels\n",
    "   - **Real-world scenario:** In production, you wouldn't have true labels, so you'd rely on:\n",
    "     - Domain expertise to interpret clusters\n",
    "     - Internal validation metrics (silhouette score, Davies-Bouldin index)\n",
    "     - Business objectives (e.g., customer segments that lead to higher sales)\n",
    "\n",
    "4. **PCA Visualization:** The 2D projection captures ~95% of variance, giving us confidence that the visualization represents the actual data structure well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions <a id='conclusions'></a>\n",
    "\n",
    "### Summary of Key Learnings\n",
    "\n",
    "#### Supervised vs. Unsupervised Learning\n",
    "\n",
    "| Aspect | Supervised Learning | Unsupervised Learning |\n",
    "|--------|---------------------|----------------------|\n",
    "| **Data** | Labeled (input-output pairs) | Unlabeled (input only) |\n",
    "| **Goal** | Predict labels for new data | Find hidden patterns/structure |\n",
    "| **Examples** | Classification, Regression | Clustering, Dimensionality Reduction |\n",
    "| **Evaluation** | Accuracy, Precision, Recall, etc. | Silhouette score, ARI (if labels available), visual inspection |\n",
    "\n",
    "#### Model Evaluation Best Practices\n",
    "\n",
    "1. **Always use train/test split** to avoid overfitting\n",
    "2. **Don't rely on accuracy alone**, especially with imbalanced data\n",
    "3. **Choose metrics based on business context:**\n",
    "   - Medical diagnosis → High recall (don't miss sick patients)\n",
    "   - Spam detection → High precision (don't block legitimate emails)\n",
    "4. **Use confusion matrix** to understand types of errors\n",
    "5. **Consider the precision-recall tradeoff** for your specific use case\n",
    "\n",
    "#### When to Use Each Approach\n",
    "\n",
    "**Use Supervised Learning when:**\n",
    "- You have labeled training data\n",
    "- You want to predict specific outcomes\n",
    "- Examples: fraud detection, image classification, price prediction\n",
    "\n",
    "**Use Unsupervised Learning when:**\n",
    "- You don't have labels or labeling is expensive\n",
    "- You want to explore data structure\n",
    "- Examples: customer segmentation, anomaly detection, data compression\n",
    "\n",
    "### Next Steps for Further Learning\n",
    "\n",
    "1. **Explore more algorithms:**\n",
    "   - Supervised: SVM, Neural Networks, Gradient Boosting\n",
    "   - Unsupervised: DBSCAN, Hierarchical Clustering, Autoencoders\n",
    "\n",
    "2. **Advanced topics:**\n",
    "   - Cross-validation for robust evaluation\n",
    "   - Hyperparameter tuning (GridSearch, RandomSearch)\n",
    "   - Feature engineering and selection\n",
    "   - Handling imbalanced datasets (SMOTE, class weights)\n",
    "\n",
    "3. **Practice with real datasets:**\n",
    "   - Kaggle competitions\n",
    "   - UCI Machine Learning Repository\n",
    "   - Your own domain-specific problems\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed this introduction to supervised and unsupervised learning. Keep practicing and experimenting with different datasets and algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference: All metrics at a glance\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL SUMMARY - METRICS AT A GLANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSUPERVISED LEARNING (Breast Cancer Classification):\")\n",
    "print(f\"  Model: Logistic Regression\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  Test samples: {len(y_test)}\")\n",
    "print(\"\\nUNSUPERVISED LEARNING (Iris Clustering):\")\n",
    "print(f\"  Algorithm: K-Means (k=3)\")\n",
    "print(f\"  Adjusted Rand Index: {ari_score:.4f}\")\n",
    "print(f\"  PCA variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"  Total samples: {len(cluster_labels)}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}